{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [NLTK](https://www.nltk.org/)\n",
    "NLTK는 영어 NLP의 시작점과도 같은 라이브러리입니다. <br>\n",
    "NLTK Tutorial(링크 missing)을 바탕으로 실습을 구성하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "필요한 데이터를 내려받기.\n",
    "본 셀의 코드를 실행하면 NLTK Downloader란 별도 창이 열립니다.\n",
    "Identifier 가운데 book을 선택한 뒤 Download 버튼을 누릅니다.\n",
    "정상적으로 작동하면 다운로드 바가 진행됩니다.\n",
    "Status가 installed로 바뀌면 완료입니다.\n",
    "완료 뒤 창을 종료(Ctrl+X)합니다.\n",
    "'''\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK methods\n",
    "NLTK methods로 NLP를 통해 인간의 문서에 대해 무엇을 알 수 있는지 예시합니다.<br>\n",
    "워밍업입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "concordance를 통해\n",
    "특정 단어가 해당 문서에서 어떤 문맥으로 나타나는지 살펴볼 수 있습니다.\n",
    "text1 = Moby Dick\n",
    "'''\n",
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "문서가 달라지면 같은 단어의 맥락도 어떻게 달라지는지 살펴보십시오.\n",
    "'''\n",
    "text2.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .similar\n",
    "특정 단어가 해당 텍스트에서 어떤 단어의 의미로 쓰였는지 추출해 줍니다<br>\n",
    "역시 그 텍스트의 성격을 드러내 주기도 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"monstrous\") # 모비딕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.similar(\"monstrous\") # 이성과 감성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시각화\n",
    "Matplotlib는 파이썬의 기본적인 시각화 라이브러리입니다.<br>\n",
    "Anaconda 설치시 함께 설치됩니다.<br>\n",
    "텍스트 데이터 시각화에 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"Amercia\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 단어가 텍스트의 어디에 분포하고 있는지를 보여줍니다<br>\n",
    "text4는 미국 대통령 취임사입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 생성\n",
    "NLP 기술을 통해 문서의 단어 분포 특성을 살려서 문장을 생성하는 것도 가능합니다<br>\n",
    "text3는 창세기입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정량적으로 살펴보기\n",
    "텍스트의 기본적인 통계는 길이입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중복 되는 단어를 제거한 unique한 단어의 수는 몇 개일까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_text3 = set(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 두 숫자를 활용하면 어떤 텍스트가 얼마나 어휘의 다양성을 가지고 있는지 알아볼 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_text3) / len(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 단어에 집중해 해당 단어가 얼마나 등장하는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3.count(\"smote\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 전체 단어 중에서 비율은 얼마나 되는지 등을 계산할 수도 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * text4.count('freedom') / len(text4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수 활용\n",
    "함수로 정의하면 더 활용하기 쉽습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "def percentage(count, total):\n",
    "    return 100 * count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_diversity(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_diversity(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage(text4.count('a'), len(text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 자연어처리 - KoNLPy \n",
    "그러면 한글에 적용해 봅시다.<br>\n",
    "파이썬 한글 자연어처리의 필수 라이브러리: [KoNLPy(코엔엘파이)](https://konlpy.org/ko/v0.5.2/) <br>\n",
    "* [설치하기](https://konlpy.org/ko/v0.5.2/install/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kolaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids = kolaw.fileids()\n",
    "fobj = kolaw.open(fids[0])\n",
    "print(fobj.read(150))\n",
    "kotext = fobj.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kotext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 조금 살펴보면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(kotext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(kotext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토크나이저 Tokenizer\n",
    "무언가 이상합니다. <br>\n",
    "단어별로 추출되는 것이 아니라 '문자'별로 추출되는 것이네요 <br>\n",
    "<br>\n",
    "각 단어별로 나누는 기능이 필요합니다.<br> \n",
    "이것이 토크나이저(tokenizer)입니다.<br>\n",
    "* 토큰 = 자연어처리에 있어서 분석자의 필요에 맞게 의미 단위로 텍스트를 쪼갰을 때 각 한 단위를 의미.<br>대표적인 단위가 단어이나, 어구, 문장 등이 될 수도 있음<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석\n",
    "이를 위해선 텍스트의 형태소 분석이 필요합니다 <br><br>\n",
    "**형태소란?** 뜻을 가진 말의 가장 작은 단위 <br>\n",
    "형태소 분석이란 이렇게 말을 최소 단위로 쪼개는 것을 말합니다 <br>\n",
    "<br>\n",
    "예를 들어 *아버지가 방에 들어가신다*  => *아버지/가/방/에/들어가/시/ㄴ다* <br>\n",
    "<br>\n",
    "표현 형태로만 보면 언어의 중의적인 특성을 생각했을 때 이것이 컴퓨터에게 쉬운 일만은 아니라는 것을 알 수 있습니다.<br>\n",
    "*나는 코끼리가 되고 싶어*라고 했을 때 *날아다니는 코끼리*가 되고 싶은 것인지 *'나'라는 화자*가 코끼리가 되고 싶은 것인지<br>\n",
    "쓰여진 텍스트만 봐서는 알 수 없는 것과 같은 문제 등이 있습니다.<br>\n",
    "<br>\n",
    "이에 대한 분석이 끝나야 원하는 토큰 단위로 텍스트를 쪼갤 수 있습니다<br>\n",
    "KoNLPy에는 5개의 형태소 분석기가 포함되어 있습니다.<br>\n",
    "각 분석기에 대한 평가는 다음과 같습니다: [링크](https://konlpy.org/ko/v0.5.2/morph/) <br>\n",
    "본 실습에선 Okt 분석기를 사용해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt=Okt()\n",
    "print(okt.morphs(kotext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kotkn = okt.morphs(kotext)\n",
    "len(kotkn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 다양성을 계산해 보면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_diversity(kotkn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석이 되고 나면 NLTK의 메소드를 활용해 텍스트를 살펴볼 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_nltk = nltk.Text(kotkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_nltk.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어의 분포\n",
    "텍스트를 살펴보는 유용한 방법중에 하나는 어떤 단어들로 이루어져 있는지를 살펴 보는 것입니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(ko_nltk)\n",
    "fdist1.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'의', '.'과 같이 의미가 작은 단어들이 상위에 랭크되어 있습니다<br>\n",
    "분석자에게 의미가 거의 없는 단어들이 **불용어**입니다<br>\n",
    "불용어에 대해선 뒤에 보다 자세히 다루고 우선 빈도를 추출해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1['기타']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에러는 한글 폰트가 설정되어 있지 않기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc, font_manager\n",
    "font_path = 'c:\\\\windows\\\\fonts\\\\malgun.ttf' #실제 운영체제의 폰트 폴더에서 마음에 드는 폰트를 고릅니다\n",
    "font = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가로 konlpy의 의안 데이터(kobill)를 활용 예제.<br>\n",
    "레퍼런스: [lucypark님 블로그](https://www.lucypark.kr/courses/2015-dm/text-mining.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill\n",
    "files_ko = kobill.fileids()\n",
    "doc_ko = kobill.open('1809890.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "tokens_ko = okt.morphs(doc_ko)\n",
    "len(tokens_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = nltk.Text(tokens_ko, name='대한민국 국회 의안 제 1809890호')\n",
    "ko.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 크기 키우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기타\n",
    "특히 긴 단어를 통해 문서의 특징을 살펴볼 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = set(text1)\n",
    "long_words = [w for w in V if len(w) > 15]\n",
    "sorted(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "글쓴이가 사용한 단어의 평균 길이를 살펴보는 것도 텍스트에 대한 정보가 될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_words = [len(w) for w in text1]\n",
    "sum(len_of_words) / len(len_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(len(w) for w in text1)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 많은 글자 수는 몇개?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.freq(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
