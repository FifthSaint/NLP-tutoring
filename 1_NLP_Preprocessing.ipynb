{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 다루기 : NLTK\n",
    "파이썬에서 텍스트 데이트를 어떻게 다루는지 연마합니다.<br>\n",
    "우선 [NLTK(Natural Language Tool Kit)](https://www.nltk.org/) 라이브러리를 활용합니다.<br>\n",
    "NLTK는 Python에서 언어 데이터를 다루는 출발점과 같은 라이브러리입니다.<br>\n",
    "NLTK가 다루는 언어는 영어입니다.<br>\n",
    "데이터를 마련해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### type\n",
    "어떤 변수에 담겨 있는 데이터가 어떤 형태인지 수시로 쓰이는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK 메소드\n",
    "NLTK 메소드를 통해 <br>\n",
    "어떤 관점에서 대상 텍스트를 파악하고자 하는지<br>\n",
    "다른 텍스트와 비교를 통해 어떤 것을 알 수 있는지<br>\n",
    "직접 살펴보겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .concordance\n",
    "이 메소드는 특정 단어가 어떤 문맥에서 쓰였는지 모아서 보여줍니다<br>\n",
    "단어의 성격 파악 뿐 아니라 해당 텍스트에서 해당 단어를 어떻게 바라보는지도 가늠하게 해줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .similar\n",
    "특정 단어가 해당 텍스트에서 어떤 단어의 의미로 쓰였는지 추출해 줍니다<br>\n",
    "역시 그 텍스트의 성격을 드러내 주기도 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"monstrous\") # 모비딕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.similar(\"monstrous\") # 이성과 감성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시각화\n",
    "Matplotlib는 파이썬의 대표적인 시각화 라이브러리입니다.<br>\n",
    "Matplotlib을 활용해 텍스트 데이터를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"Amercia\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 단어가 텍스트의 어디에 분포하고 있는지를 보여줍니다<br>\n",
    "text4는 미국 대통령 취임사입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 생성\n",
    "NLTK는 내장된 텍스트 생성 기능이 있습니다<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정량적으로 살펴보기\n",
    "텍스트의 기본적인 통계는 길이입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중복 되는 단어를 제거한 unique한 단어의 수는 몇 개일까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_text3 = set(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 두 숫자를 활용하면 어떤 텍스트가 얼마나 어휘의 다양성을 가지고 있는지 알아볼 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_text3) / len(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 단어에 집중해 해당 단어가 얼마나 등장하는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3.count(\"smote\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 전체 단어 중에서 비율은 얼마나 되는지 등을 계산할 수도 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * text4.count('freedom') / len(text4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 함수 정의\n",
    "이런 다양성을 함수로 정의해 봅니다<br>\n",
    "그러면 활용하기 더욱 수월하죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "def percentage(count, total):\n",
    "    return 100 * count / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_diversity(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_diversity(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage(text4.count('a'), len(text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 자연어처리 - KoNLPy \n",
    "그러면 한글에 적용해 봅시다.<br>\n",
    "파이썬 한글 자연어처리의 필수 라이브러리: [KoNLPy(코엔엘파이)](https://konlpy.org/ko/v0.5.2/) <br>\n",
    "* [설치하기](https://konlpy.org/ko/v0.5.2/install/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kolaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids = kolaw.fileids()\n",
    "fobj = kolaw.open(fids[0])\n",
    "print(fobj.read(150))\n",
    "kotext = fobj.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kotext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 조금 살펴보면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(kotext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(kotext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토크나이저 Tokenizer\n",
    "무언가 이상합니다. <br>\n",
    "단어별로 추출되는 것이 아니라 '문자'별로 추출되는 것이네요 <br>\n",
    "<br>\n",
    "각 단어별로 나누는 기능이 필요.<br> \n",
    "이것이 토크나이저(tokenizer)입니다.<br>\n",
    "* 토큰이란? 토큰은 정해진 개념이라기보다 자연어처리에 있어서 분석자의 필요에 맞게 의미 단위로 텍스트를 쪼갰을 때<br>각 한 단위를 의미합니다. 보통 대표적인 단위가 단어.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석\n",
    "이를 위해선 텍스트의 형태소 분석이 필요합니다 <br><br>\n",
    "**형태소란?** 뜻을 가진 말의 가장 작은 단위 <br>\n",
    "형태소 분석이란 이렇게 말을 최소 단위로 쪼개는 것을 말합니다 <br>\n",
    "<br>\n",
    "예를 들어 *아버지가 방에 들어가신다*  => *아버지/가/방/에/들어가/시/ㄴ다* <br>\n",
    "<br>\n",
    "표현 형태로만 보면 언어의 중의적인 특성을 생각했을 때 이것이 컴퓨터에게 쉬운 일만은 아니라는 것을 알 수 있습니다.<br>\n",
    "*나는 코끼리가 되고 싶어*라고 했을 때 *날아다니는 코끼리*가 되고 싶은 것인지 *'나'라는 화자*가 코끼리가 되고 싶은 것인지<br>\n",
    "쓰여진 텍스트만 봐서는 알 수 없는 것과 같은 문제 등이 있는 것이지요.<br>\n",
    "<br>\n",
    "이런 분석이 끝나고 나야 원하는 토큰(지금 경우 단어) 단위로 텍스트를 쪼갤 수 있습니다<br>\n",
    "koNLPy는 5개의 형태소 분석기를 합친 라이브러리입니다.<br>\n",
    "각 분석기에 대한 평가는 다음과 같습니다: [링크](https://konlpy.org/ko/v0.5.2/morph/) <br>\n",
    "지금은 편의상 무난한 Okt 분석기를 쓰도록 하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt=Okt()\n",
    "print(okt.morphs(kotext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kotkn = okt.morphs(kotext)\n",
    "len(kotkn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 원했던 다양성을 계산해 보면..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(kotkn)) / len(kotkn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석이 되고 나면 nltk 텍스트로 다음과 같이 바꿔서 앞서 메소드들을 활용할 수도 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_nltk = nltk.Text(kotkn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_nltk.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트를 살펴보는 유용한 방법중에 하나는 어떤 단어들로 이루어져 있는지를 살펴 보는 것.<br>\n",
    "단어의 통계."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(ko_nltk)\n",
    "fdist1.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상당히 쓸모 없는 단어들이 상위에 많이 랭크되어 있습니다<br>\n",
    "이런 단어들을 **불용어**라고 합니다<br>\n",
    "이런 단어들을 정리하는 방법은 뒤에 다루고 일단 여기는 빈도추출 개념에 대해서만 보고 지나가겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1['기타']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에러는 한글 폰트가 설정되어 있지 않기 때문입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc, font_manager\n",
    "rc('font', family='NanumGothic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 konlpy의 의안 데이터(kobill)를 활용해 보겠습니다 <br>\n",
    "레퍼런스: [lucypark님 블로그](https://www.lucypark.kr/courses/2015-dm/text-mining.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill\n",
    "files_ko = kobill.fileids()\n",
    "doc_ko = kobill.open('1809890.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화를 시켜야"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ko = okt.morphs(doc_ko)\n",
    "len(tokens_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = nltk.Text(tokens_ko, name='대한민국 국회 의안 제 1809890호')\n",
    "ko.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 크기 키우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그밖에.. 해당 텍스트의 특성을 살펴보기 위해<br>\n",
    "특히 긴 단어들을 보면 어떨까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = set(text1)\n",
    "long_words = [w for w in V if len(w) > 15]\n",
    "sorted(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 길이들을 살펴보는 것도 텍스트에 대한 유용한 정보가 될 수 있음<br>\n",
    "평균적으로 얼마나 긴 단어들을 사용하는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(w) for w in text1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글 폰트 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rc, font_manager\n",
    "rc('font', family='NanumGothic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(len(w) for w in text1)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 많은 글자 수는 몇개?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.freq(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(추가)** 한글 의안의 경우 어떤 글자수의 단어가 가장 많이 쓰였을까? 그리고 그 비중은? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
